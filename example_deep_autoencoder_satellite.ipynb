{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"autoencoder_deep.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"3SegMef22EMs","colab_type":"code","outputId":"6e2eb003-f660-4073-95c5-e8cca82d2064","executionInfo":{"status":"ok","timestamp":1572722985000,"user_tz":180,"elapsed":2848,"user":{"displayName":"Ana Lima","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mD_rVEqxF9pNV2UmDCr1wJi96KOZtAgLTRMQ_YImw=s64","userId":"16351871197992508535"}},"colab":{"base_uri":"https://localhost:8080/","height":64}},"source":["from tensorflow.keras.layers import Dense, Input\n","from tensorflow.keras.models import Model, Sequential\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import numpy as np\n","from math import *\n","import os"],"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"u3icAOFA2HBi","colab_type":"code","colab":{}},"source":["def deep_autoencoder(input_dim, encoded_dim, x_train, x_test, my_image_test):\n","    autoencoder = Sequential()\n","\n","    # Encoder Layers\n","    autoencoder.add(Dense(4 * encoded_dim, input_shape=(input_dim,), activation='relu'))\n","    autoencoder.add(Dense(2 * encoded_dim, activation='relu'))\n","    autoencoder.add(Dense(encoded_dim, activation='relu'))\n","\n","    # Decoder Layers\n","    autoencoder.add(Dense(2 * encoded_dim, activation='relu'))\n","    autoencoder.add(Dense(4 * encoded_dim, activation='relu'))\n","    autoencoder.add(Dense(input_dim, activation='sigmoid'))\n","            \n","    input_img = Input(shape=(input_dim,))\n","    encoder_layer1 = autoencoder.layers[0]\n","    encoder_layer2 = autoencoder.layers[1]\n","    encoder_layer3 = autoencoder.layers[2]\n","    encoder = Model(input_img, encoder_layer3(encoder_layer2(encoder_layer1(input_img))))\n","    \n","    autoencoder.summary()\n","    encoder.summary()\n","    \n","    autoencoder.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    autoencoder.fit(x_train, x_train,\n","                epochs=3500,\n","                batch_size=64,\n","                shuffle=True,\n","                validation_data=(x_test, x_test))\n","    \n","    encoded_imgs = encoder.predict(x_test)\n","    decoded_imgs = autoencoder.predict(x_test)\n","    \n","    return (encoded_imgs, decoded_imgs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ddOpX-G62Jbo","colab_type":"code","colab":{}},"source":["DATADIR = './data'\n","#CATEGORIES = ['test', 'train']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y8F-Isoh2MOL","colab_type":"code","outputId":"ce54f054-3864-43c2-b7d5-f877761a7779","executionInfo":{"status":"ok","timestamp":1572384875504,"user_tz":180,"elapsed":6010275,"user":{"displayName":"Ana Lima","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mD_rVEqxF9pNV2UmDCr1wJi96KOZtAgLTRMQ_YImw=s64","userId":"16351871197992508535"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["for img in os.listdir(DATADIR):\n","    try:      \n","        # my_image_original = mpimg.imread(os.path.join(path, img))\n","        my_image_original = mpimg.imread((os.path.join(DATADIR, img)))\n","        max_original_value = float(my_image_original.max())\n","        x_original = my_image_original.astype('float32') / max_original_value\n","        x_original = x_original.reshape((len(x_original), np.prod(x_original.shape[1:])))\n","\n","        # my_image_test = mpimg.imread(os.path.join(path, img))\n","        # max_train_test = float(my_image_train.max())\n","        # x_test = my_image_test.astype('float32') / max_train_test\n","        # x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n","    \n","        (encoded_imgs, decoded_imgs) = deep_autoencoder(x_original.shape[1], 64, x_original, x_original, my_image_original)  \n","        \n","        to_reshape = my_image_original.shape\n","        x_original_result = x_original.reshape(to_reshape[0], to_reshape[1], to_reshape[2])\n","        decoded_imgs_result = decoded_imgs.reshape(to_reshape[0], to_reshape[1], to_reshape[2])      \n","        # x_test_result = x_original.reshape(to_reshape[0], to_reshape[1], to_reshape[2])\n","\n","\n","        plt.axis(\"off\")\n","\n","        plt.figure()\n","        # plt.subplot(3, 1, img+1)\n","        imgplot_original_result = plt.imshow(x_original_result)\n","        plt.title('Train image')\n","        mpimg.imsave('./result/' + img + '_original.png', my_image_original)\n","\n","        plt.figure()\n","        # plt.subplot(3, 1, img+1)\n","        imgplot_encoded_result = plt.imshow(encoded_imgs)\n","        plt.title('Encoded image')\n","        mpimg.imsave('./result/' + img + '_encoded_imgs.png', encoded_imgs)\n","\n","        plt.figure()\n","        # plt.subplot(3, 1, img+1)\n","        imgplot_decoded_imgs = plt.imshow(decoded_imgs_result)\n","        plt.title('Decoded image')\n","        mpimg.imsave('./result/' + img + '_decoded_imgs.png', decoded_imgs_result)\n","\n","    except Exception as e:\n","        pass"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense (Dense)                (None, 256)               6145280   \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 128)               32896     \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 64)                8256      \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 128)               8320      \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 256)               33024     \n","_________________________________________________________________\n","dense_5 (Dense)              (None, 24004)             6169028   \n","=================================================================\n","Total params: 12,396,804\n","Trainable params: 12,396,804\n","Non-trainable params: 0\n","_________________________________________________________________\n","Model: \"model\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         [(None, 24004)]           0         \n","_________________________________________________________________\n","dense (Dense)                (None, 256)               6145280   \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 128)               32896     \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 64)                8256      \n","=================================================================\n","Total params: 6,186,432\n","Trainable params: 6,186,432\n","Non-trainable params: 0\n","_________________________________________________________________\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","Train on 5501 samples, validate on 5501 samples\n","Epoch 1/3500\n","5501/5501 [==============================] - 3s 606us/sample - loss: 0.4907 - acc: 0.2493 - val_loss: 0.4751 - val_acc: 0.2523\n","Epoch 2/3500\n","5501/5501 [==============================] - 2s 445us/sample - loss: 0.4716 - acc: 0.2523 - val_loss: 0.4671 - val_acc: 0.2523\n","Epoch 3/3500\n","5501/5501 [==============================] - 2s 449us/sample - loss: 0.4644 - acc: 0.2523 - val_loss: 0.4616 - val_acc: 0.2523\n","Epoch 4/3500\n","5501/5501 [==============================] - 2s 446us/sample - loss: 0.4603 - acc: 0.2523 - val_loss: 0.4588 - val_acc: 0.2523\n","Epoch 5/3500\n","5501/5501 [==============================] - 2s 448us/sample - loss: 0.4582 - acc: 0.2523 - val_loss: 0.4574 - val_acc: 0.2523\n","Epoch 6/3500\n","5501/5501 [==============================] - 2s 448us/sample - loss: 0.4572 - acc: 0.2523 - val_loss: 0.4564 - val_acc: 0.2523\n","Epoch 7/3500\n","5501/5501 [==============================] - 2s 447us/sample - loss: 0.4561 - acc: 0.2523 - val_loss: 0.4556 - val_acc: 0.2523\n","Epoch 8/3500\n","5501/5501 [==============================] - 2s 445us/sample - loss: 0.4555 - acc: 0.2523 - val_loss: 0.4551 - val_acc: 0.2523\n","Epoch 9/3500\n","5501/5501 [==============================] - 2s 451us/sample - loss: 0.4549 - acc: 0.2523 - val_loss: 0.4544 - val_acc: 0.2523\n","Epoch 10/3500\n","5501/5501 [==============================] - 2s 448us/sample - loss: 0.4543 - acc: 0.2523 - val_loss: 0.4539 - val_acc: 0.2523\n","Epoch 11/3500\n","5501/5501 [==============================] - 2s 446us/sample - loss: 0.4540 - acc: 0.2523 - val_loss: 0.4536 - val_acc: 0.2523\n","Epoch 12/3500\n","5501/5501 [==============================] - 2s 448us/sample - loss: 0.4537 - acc: 0.2523 - val_loss: 0.4534 - val_acc: 0.2523\n","Epoch 13/3500\n","5501/5501 [==============================] - 2s 445us/sample - loss: 0.4535 - acc: 0.2523 - val_loss: 0.4532 - val_acc: 0.2523\n","Epoch 14/3500\n","5501/5501 [==============================] - 2s 447us/sample - loss: 0.4531 - acc: 0.2523 - val_loss: 0.4528 - val_acc: 0.2523\n","Epoch 15/3500\n","5501/5501 [==============================] - 2s 445us/sample - loss: 0.4529 - acc: 0.2523 - val_loss: 0.4525 - val_acc: 0.2523\n","Epoch 16/3500\n","5501/5501 [==============================] - 2s 446us/sample - loss: 0.4527 - acc: 0.2523 - val_loss: 0.4526 - val_acc: 0.2523\n","Epoch 17/3500\n","5501/5501 [==============================] - 2s 446us/sample - loss: 0.4525 - acc: 0.2523 - val_loss: 0.4522 - val_acc: 0.2523\n","Epoch 18/3500\n","5501/5501 [==============================] - 2s 449us/sample - loss: 0.4523 - acc: 0.2523 - val_loss: 0.4520 - val_acc: 0.2523\n","Epoch 19/3500\n","5501/5501 [==============================] - 2s 450us/sample - loss: 0.4522 - acc: 0.2523 - val_loss: 0.4519 - val_acc: 0.2523\n","Epoch 20/3500\n","5501/5501 [==============================] - 2s 452us/sample - loss: 0.4520 - acc: 0.2523 - val_loss: 0.4519 - val_acc: 0.2523\n","Epoch 21/3500\n","5501/5501 [==============================] - 2s 446us/sample - loss: 0.4519 - acc: 0.2523 - val_loss: 0.4518 - val_acc: 0.2523\n","Epoch 22/3500\n","5501/5501 [==============================] - 2s 443us/sample - loss: 0.4519 - acc: 0.2523 - val_loss: 0.4515 - val_acc: 0.2523\n","Epoch 23/3500\n","5501/5501 [==============================] - 2s 445us/sample - loss: 0.4517 - acc: 0.2523 - val_loss: 0.4516 - val_acc: 0.2523\n","Epoch 24/3500\n","5501/5501 [==============================] - 2s 449us/sample - loss: 0.4515 - acc: 0.2523 - val_loss: 0.4512 - val_acc: 0.2523\n","Epoch 25/3500\n","5501/5501 [==============================] - 2s 442us/sample - loss: 0.4514 - acc: 0.2523 - val_loss: 0.4512 - val_acc: 0.2523\n","Epoch 26/3500\n","5501/5501 [==============================] - 2s 449us/sample - loss: 0.4513 - acc: 0.2523 - val_loss: 0.4513 - val_acc: 0.2523\n","Epoch 27/3500\n","5501/5501 [==============================] - 2s 447us/sample - loss: 0.4512 - acc: 0.2523 - val_loss: 0.4513 - val_acc: 0.2523\n","Epoch 28/3500\n","5501/5501 [==============================] - 2s 449us/sample - loss: 0.4512 - acc: 0.2523 - val_loss: 0.4509 - val_acc: 0.2523\n","Epoch 29/3500\n","5501/5501 [==============================] - 2s 451us/sample - loss: 0.4510 - acc: 0.2523 - val_loss: 0.4508 - val_acc: 0.2523\n","Epoch 30/3500\n","5501/5501 [==============================] - 2s 450us/sample - loss: 0.4509 - acc: 0.2523 - val_loss: 0.4506 - val_acc: 0.2523\n","Epoch 31/3500\n","5501/5501 [==============================] - 2s 450us/sample - loss: 0.4508 - acc: 0.2523 - val_loss: 0.4506 - val_acc: 0.2523\n","Epoch 32/3500\n","5501/5501 [==============================] - 2s 450us/sample - loss: 0.4510 - acc: 0.2523 - val_loss: 0.4506 - val_acc: 0.2523\n","Epoch 33/3500\n","5501/5501 [==============================] - 2s 447us/sample - loss: 0.4507 - acc: 0.2523 - val_loss: 0.4505 - val_acc: 0.2523\n","Epoch 34/3500\n","5501/5501 [==============================] - 2s 447us/sample - loss: 0.4507 - acc: 0.2523 - val_loss: 0.4505 - val_acc: 0.2523\n","Epoch 35/3500\n","5501/5501 [==============================] - 2s 445us/sample - loss: 0.4506 - acc: 0.2523 - val_loss: 0.4504 - val_acc: 0.2523\n","Epoch 36/3500\n","5501/5501 [==============================] - 2s 447us/sample - loss: 0.4505 - acc: 0.2523 - val_loss: 0.4504 - val_acc: 0.2523\n","Epoch 37/3500\n","5501/5501 [==============================] - 2s 450us/sample - loss: 0.4505 - acc: 0.2523 - val_loss: 0.4503 - val_acc: 0.2523\n","Epoch 38/3500\n","5501/5501 [==============================] - 2s 445us/sample - loss: 0.4504 - acc: 0.2523 - val_loss: 0.4503 - val_acc: 0.2523\n","Epoch 39/3500\n","5501/5501 [==============================] - 2s 448us/sample - loss: 0.4503 - acc: 0.2523 - val_loss: 0.4501 - val_acc: 0.2523\n","Epoch 40/3500\n","5501/5501 [==============================] - 2s 444us/sample - loss: 0.4503 - acc: 0.2523 - val_loss: 0.4502 - val_acc: 0.2523\n","Epoch 41/3500\n","5501/5501 [==============================] - 2s 448us/sample - loss: 0.4503 - acc: 0.2523 - val_loss: 0.4502 - val_acc: 0.2523\n","Epoch 42/3500\n","5501/5501 [==============================] - 2s 451us/sample - loss: 0.4503 - acc: 0.2523 - val_loss: 0.4500 - val_acc: 0.2523\n","Epoch 43/3500\n","5501/5501 [==============================] - 2s 446us/sample - loss: 0.4502 - acc: 0.2523 - val_loss: 0.4502 - val_acc: 0.2523\n","Epoch 44/3500\n","5501/5501 [==============================] - 2s 447us/sample - loss: 0.4503 - acc: 0.2523 - val_loss: 0.4499 - val_acc: 0.2523\n","Epoch 45/3500\n","5501/5501 [==============================] - 2s 446us/sample - loss: 0.4500 - acc: 0.2523 - val_loss: 0.4500 - val_acc: 0.2523\n","Epoch 46/3500\n","5501/5501 [==============================] - 2s 447us/sample - loss: 0.4500 - acc: 0.2523 - val_loss: 0.4499 - val_acc: 0.2523\n","Epoch 47/3500\n","5501/5501 [==============================] - 2s 445us/sample - loss: 0.4500 - acc: 0.2523 - val_loss: 0.4498 - val_acc: 0.2523\n","Epoch 48/3500\n","5501/5501 [==============================] - 2s 449us/sample - loss: 0.4500 - acc: 0.2523 - val_loss: 0.4498 - val_acc: 0.2523\n","Epoch 49/3500\n","5501/5501 [==============================] - 2s 444us/sample - loss: 0.4499 - acc: 0.2523 - val_loss: 0.4498 - val_acc: 0.2523\n","Epoch 50/3500\n","5501/5501 [==============================] - 2s 448us/sample - loss: 0.4499 - acc: 0.2523 - val_loss: 0.4498 - val_acc: 0.2523\n","Epoch 51/3500\n","5501/5501 [==============================] - 2s 443us/sample - loss: 0.4499 - acc: 0.2523 - val_loss: 0.4500 - val_acc: 0.2523\n","Epoch 52/3500\n","5501/5501 [==============================] - 2s 443us/sample - loss: 0.4498 - acc: 0.2523 - val_loss: 0.4496 - val_acc: 0.2523\n","Epoch 53/3500\n","5501/5501 [==============================] - 2s 445us/sample - loss: 0.4497 - acc: 0.2523 - val_loss: 0.4495 - val_acc: 0.2523\n","Epoch 54/3500\n","5501/5501 [==============================] - 2s 446us/sample - loss: 0.4498 - acc: 0.2523 - val_loss: 0.4497 - val_acc: 0.2523\n","Epoch 55/3500\n","5501/5501 [==============================] - 2s 448us/sample - loss: 0.4498 - acc: 0.2523 - val_loss: 0.4499 - val_acc: 0.2523\n","Epoch 56/3500\n","5501/5501 [==============================] - 2s 445us/sample - loss: 0.4497 - acc: 0.2523 - val_loss: 0.4495 - val_acc: 0.2523\n","Epoch 57/3500\n","5501/5501 [==============================] - 2s 445us/sample - loss: 0.4497 - acc: 0.2523 - val_loss: 0.4496 - val_acc: 0.2523\n","Epoch 58/3500\n","5501/5501 [==============================] - 2s 445us/sample - loss: 0.4496 - acc: 0.2523 - val_loss: 0.4495 - val_acc: 0.2523\n","Epoch 59/3500\n","5501/5501 [==============================] - 2s 444us/sample - loss: 0.4496 - acc: 0.2523 - val_loss: 0.4496 - val_acc: 0.2523\n","Epoch 60/3500\n","5501/5501 [==============================] - 2s 446us/sample - loss: 0.4496 - acc: 0.2523 - val_loss: 0.4495 - val_acc: 0.2523\n","Epoch 61/3500\n","5501/5501 [==============================] - 2s 444us/sample - loss: 0.4495 - acc: 0.2523 - val_loss: 0.4495 - val_acc: 0.2523\n","Epoch 62/3500\n","5501/5501 [==============================] - 2s 447us/sample - loss: 0.4496 - acc: 0.2523 - val_loss: 0.4497 - val_acc: 0.2523\n","Epoch 63/3500\n","5501/5501 [==============================] - 2s 444us/sample - loss: 0.4496 - acc: 0.2523 - val_loss: 0.4496 - val_acc: 0.2523\n","Epoch 64/3500\n","5501/5501 [==============================] - 2s 444us/sample - loss: 0.4495 - acc: 0.2523 - val_loss: 0.4493 - val_acc: 0.2523\n","Epoch 65/3500\n","5501/5501 [==============================] - 2s 444us/sample - loss: 0.4495 - acc: 0.2523 - val_loss: 0.4493 - val_acc: 0.2523\n","Epoch 66/3500\n","5501/5501 [==============================] - 2s 444us/sample - loss: 0.4494 - acc: 0.2523 - val_loss: 0.4493 - val_acc: 0.2523\n","Epoch 67/3500\n","5501/5501 [==============================] - 2s 445us/sample - loss: 0.4496 - acc: 0.2523 - val_loss: 0.4494 - val_acc: 0.2523\n","Epoch 68/3500\n","5501/5501 [==============================] - 2s 449us/sample - loss: 0.4494 - acc: 0.2523 - val_loss: 0.4492 - val_acc: 0.2523\n","Epoch 69/3500\n","5501/5501 [==============================] - 2s 447us/sample - loss: 0.4494 - acc: 0.2523 - val_loss: 0.4494 - val_acc: 0.2523\n","Epoch 70/3500\n","5501/5501 [==============================] - 2s 447us/sample - loss: 0.4494 - acc: 0.2523 - val_loss: 0.4494 - val_acc: 0.2523\n","Epoch 71/3500\n","5501/5501 [==============================] - 2s 445us/sample - loss: 0.4495 - acc: 0.2523 - val_loss: 0.4493 - val_acc: 0.2523\n","Epoch 72/3500\n","5501/5501 [==============================] - 2s 450us/sample - loss: 0.4495 - acc: 0.2523 - val_loss: 0.4493 - val_acc: 0.2523\n","Epoch 73/3500\n","5501/5501 [==============================] - 2s 450us/sample - loss: 0.4493 - acc: 0.2523 - val_loss: 0.4491 - val_acc: 0.2523\n","Epoch 74/3500\n","5501/5501 [==============================] - 2s 442us/sample - loss: 0.4493 - acc: 0.2523 - val_loss: 0.4493 - val_acc: 0.2523\n","Epoch 75/3500\n","5501/5501 [==============================] - 2s 453us/sample - loss: 0.4493 - acc: 0.2523 - val_loss: 0.4493 - val_acc: 0.2523\n","Epoch 76/3500\n","5501/5501 [==============================] - 2s 449us/sample - loss: 0.4493 - acc: 0.2523 - val_loss: 0.4491 - val_acc: 0.2523\n","Epoch 77/3500\n","5501/5501 [==============================] - 3s 455us/sample - loss: 0.4493 - acc: 0.2523 - val_loss: 0.4492 - val_acc: 0.2523\n","Epoch 78/3500\n","5501/5501 [==============================] - 2s 450us/sample - loss: 0.4492 - acc: 0.2523 - val_loss: 0.4491 - val_acc: 0.2523\n","Epoch 79/3500\n","5501/5501 [==============================] - 2s 449us/sample - loss: 0.4493 - acc: 0.2523 - val_loss: 0.4492 - val_acc: 0.2523\n","Epoch 80/3500\n","5501/5501 [==============================] - 2s 450us/sample - loss: 0.4493 - acc: 0.2523 - val_loss: 0.4491 - val_acc: 0.2523\n","Epoch 81/3500\n","5501/5501 [==============================] - 2s 447us/sample - loss: 0.4492 - acc: 0.2523 - val_loss: 0.4490 - val_acc: 0.2523\n","Epoch 82/3500\n","5501/5501 [==============================] - 2s 449us/sample - loss: 0.4493 - acc: 0.2523 - val_loss: 0.4491 - val_acc: 0.2523\n","Epoch 83/3500\n","5501/5501 [==============================] - 2s 453us/sample - loss: 0.4492 - acc: 0.2523 - val_loss: 0.4490 - val_acc: 0.2523\n","Epoch 84/3500\n","5501/5501 [==============================] - 2s 444us/sample - loss: 0.4492 - acc: 0.2523 - val_loss: 0.4490 - val_acc: 0.2523\n","Epoch 85/3500\n","5501/5501 [==============================] - 2s 448us/sample - loss: 0.4492 - acc: 0.2523 - val_loss: 0.4490 - val_acc: 0.2523\n","Epoch 86/3500\n","5501/5501 [==============================] - 2s 443us/sample - loss: 0.4491 - acc: 0.2523 - val_loss: 0.4492 - val_acc: 0.2523\n","Epoch 87/3500\n","5501/5501 [==============================] - 2s 444us/sample - loss: 0.4492 - acc: 0.2523 - val_loss: 0.4491 - val_acc: 0.2523\n","Epoch 88/3500\n","5501/5501 [==============================] - 2s 450us/sample - loss: 0.4492 - acc: 0.2523 - val_loss: 0.4491 - val_acc: 0.2523\n","Epoch 89/3500\n","5501/5501 [==============================] - 2s 447us/sample - loss: 0.4491 - acc: 0.2523 - val_loss: 0.4490 - val_acc: 0.2523\n","Epoch 90/3500\n","5501/5501 [==============================] - 2s 450us/sample - loss: 0.4492 - acc: 0.2523 - val_loss: 0.4490 - val_acc: 0.2523\n","Epoch 91/3500\n","5501/5501 [==============================] - 2s 451us/sample - loss: 0.4491 - acc: 0.2523 - val_loss: 0.4489 - val_acc: 0.2523\n","Epoch 92/3500\n","5501/5501 [==============================] - 3s 463us/sample - loss: 0.4491 - acc: 0.2523 - val_loss: 0.4490 - val_acc: 0.2523\n","Epoch 93/3500\n","5501/5501 [==============================] - 3s 458us/sample - loss: 0.4491 - acc: 0.2523 - val_loss: 0.4490 - val_acc: 0.2523\n","Epoch 94/3500\n","5501/5501 [==============================] - 3s 459us/sample - loss: 0.4491 - acc: 0.2523 - val_loss: 0.4489 - val_acc: 0.2523\n","Epoch 95/3500\n","5501/5501 [==============================] - 2s 452us/sample - loss: 0.4491 - acc: 0.2523 - val_loss: 0.4491 - val_acc: 0.2523\n","Epoch 96/3500\n","5501/5501 [==============================] - 2s 453us/sample - loss: 0.4492 - acc: 0.2523 - val_loss: 0.4490 - val_acc: 0.2523\n","Epoch 97/3500\n","5501/5501 [==============================] - 2s 450us/sample - loss: 0.4491 - acc: 0.2523 - val_loss: 0.4490 - val_acc: 0.2523\n","Epoch 98/3500\n","5501/5501 [==============================] - 2s 454us/sample - loss: 0.4490 - acc: 0.2523 - val_loss: 0.4488 - val_acc: 0.2523\n","Epoch 99/3500\n","5501/5501 [==============================] - 3s 459us/sample - loss: 0.4490 - acc: 0.2523 - val_loss: 0.4489 - val_acc: 0.2523\n","Epoch 100/3500\n","5501/5501 [==============================] - 2s 447us/sample - loss: 0.4490 - acc: 0.2523 - val_loss: 0.4489 - val_acc: 0.2523\n","Epoch 101/3500\n","5501/5501 [==============================] - 3s 455us/sample - loss: 0.4491 - acc: 0.2523 - val_loss: 0.4491 - val_acc: 0.2523\n","Epoch 102/3500\n","5501/5501 [==============================] - 2s 448us/sample - loss: 0.4491 - acc: 0.2523 - val_loss: 0.4489 - val_acc: 0.2523\n","Epoch 103/3500\n","5501/5501 [==============================] - 3s 455us/sample - loss: 0.4491 - acc: 0.2523 - val_loss: 0.4490 - val_acc: 0.2523\n","Epoch 104/3500\n","5501/5501 [==============================] - 2s 446us/sample - loss: 0.4490 - acc: 0.2523 - val_loss: 0.4488 - val_acc: 0.2523\n","Epoch 105/3500\n","5501/5501 [==============================] - 2s 450us/sample - loss: 0.4490 - acc: 0.2523 - val_loss: 0.4490 - val_acc: 0.2523\n","Epoch 106/3500\n","5501/5501 [==============================] - 2s 444us/sample - loss: 0.4491 - acc: 0.2523 - val_loss: 0.4490 - val_acc: 0.2523\n","Epoch 107/3500\n","5501/5501 [==============================] - 2s 442us/sample - loss: 0.4490 - acc: 0.2523 - val_loss: 0.4488 - val_acc: 0.2523\n","Epoch 108/3500\n","5501/5501 [==============================] - 2s 446us/sample - loss: 0.4489 - acc: 0.2523 - val_loss: 0.4490 - val_acc: 0.2523\n","Epoch 109/3500\n","5501/5501 [==============================] - 2s 447us/sample - loss: 0.4491 - acc: 0.2523 - val_loss: 0.4489 - val_acc: 0.2523\n","Epoch 110/3500\n","5501/5501 [==============================] - 2s 440us/sample - loss: 0.4490 - acc: 0.2523 - val_loss: 0.4488 - val_acc: 0.2523\n","Epoch 111/3500\n","5501/5501 [==============================] - 2s 443us/sample - loss: 0.4490 - acc: 0.2523 - val_loss: 0.4489 - val_acc: 0.2523\n","Epoch 112/3500\n","5501/5501 [==============================] - 2s 441us/sample - loss: 0.4489 - acc: 0.2523 - val_loss: 0.4488 - val_acc: 0.2523\n","Epoch 113/3500\n","5501/5501 [==============================] - 2s 444us/sample - loss: 0.4490 - acc: 0.2523 - val_loss: 0.4488 - val_acc: 0.2523\n","Epoch 114/3500\n","5501/5501 [==============================] - 2s 443us/sample - loss: 0.4490 - acc: 0.2523 - val_loss: 0.4488 - val_acc: 0.2523\n","Epoch 115/3500\n","5501/5501 [==============================] - 2s 450us/sample - loss: 0.4490 - acc: 0.2523 - val_loss: 0.4488 - val_acc: 0.2523\n","Epoch 116/3500\n","5501/5501 [==============================] - 2s 449us/sample - loss: 0.4489 - acc: 0.2523 - val_loss: 0.4488 - val_acc: 0.2523\n","Epoch 117/3500\n","5501/5501 [==============================] - 2s 447us/sample - loss: 0.4490 - acc: 0.2523 - val_loss: 0.4488 - val_acc: 0.2523\n","Epoch 118/3500\n","5501/5501 [==============================] - 2s 447us/sample - loss: 0.4489 - acc: 0.2523 - val_loss: 0.4488 - val_acc: 0.2523\n","Epoch 119/3500\n","5501/5501 [==============================] - 2s 448us/sample - loss: 0.4489 - acc: 0.2523 - val_loss: 0.4487 - val_acc: 0.2523\n","Epoch 120/3500\n","5501/5501 [==============================] - 2s 447us/sample - loss: 0.4489 - acc: 0.2523 - val_loss: 0.4487 - val_acc: 0.2523\n","Epoch 121/3500\n","5501/5501 [==============================] - 2s 451us/sample - loss: 0.4489 - acc: 0.2523 - val_loss: 0.4488 - val_acc: 0.2523\n","Epoch 122/3500\n","5501/5501 [==============================] - 2s 452us/sample - loss: 0.4489 - acc: 0.2523 - val_loss: 0.4488 - val_acc: 0.2523\n","Epoch 123/3500\n","5501/5501 [==============================] - 2s 444us/sample - loss: 0.4489 - acc: 0.2523 - val_loss: 0.4488 - val_acc: 0.2523\n","Epoch 124/3500\n","5501/5501 [==============================] - 2s 446us/sample - loss: 0.4490 - acc: 0.2523 - val_loss: 0.4489 - val_acc: 0.2523\n","Epoch 125/3500\n","5501/5501 [==============================] - 2s 446us/sample - loss: 0.4489 - acc: 0.2523 - val_loss: 0.4487 - val_acc: 0.2523\n","Epoch 126/3500\n","5501/5501 [==============================] - 2s 447us/sample - loss: 0.4488 - acc: 0.2523 - val_loss: 0.4487 - val_acc: 0.2523\n","Epoch 127/3500\n","5501/5501 [==============================] - 2s 445us/sample - loss: 0.4489 - acc: 0.2523 - val_loss: 0.4487 - val_acc: 0.2523\n","Epoch 128/3500\n","5501/5501 [==============================] - 2s 452us/sample - loss: 0.4489 - acc: 0.2523 - val_loss: 0.4490 - val_acc: 0.2523\n","Epoch 129/3500\n","5501/5501 [==============================] - 2s 446us/sample - loss: 0.4489 - acc: 0.2523 - val_loss: 0.4487 - val_acc: 0.2523\n","Epoch 130/3500\n","5501/5501 [==============================] - 2s 450us/sample - loss: 0.4488 - acc: 0.2523 - val_loss: 0.4486 - val_acc: 0.2523\n","Epoch 131/3500\n","5501/5501 [==============================] - 2s 447us/sample - loss: 0.4488 - acc: 0.2523 - val_loss: 0.4487 - val_acc: 0.2523\n","Epoch 132/3500\n","5501/5501 [==============================] - 2s 446us/sample - loss: 0.4488 - acc: 0.2523 - val_loss: 0.4487 - val_acc: 0.2523\n","Epoch 133/3500\n","5501/5501 [==============================] - 2s 447us/sample - loss: 0.4489 - acc: 0.2523 - val_loss: 0.4487 - val_acc: 0.2523\n","Epoch 134/3500\n","5501/5501 [==============================] - 2s 449us/sample - loss: 0.4488 - acc: 0.2523 - val_loss: 0.4487 - val_acc: 0.2523\n","Epoch 135/3500\n","5501/5501 [==============================] - 2s 449us/sample - loss: 0.4488 - acc: 0.2523 - val_loss: 0.4489 - val_acc: 0.2523\n","Epoch 136/3500\n","5501/5501 [==============================] - 2s 451us/sample - loss: 0.4489 - acc: 0.2523 - val_loss: 0.4489 - val_acc: 0.2523\n","Epoch 137/3500\n","5501/5501 [==============================] - 2s 446us/sample - loss: 0.4489 - acc: 0.2523 - val_loss: 0.4487 - val_acc: 0.2523\n","Epoch 138/3500\n","5501/5501 [==============================] - 2s 447us/sample - loss: 0.4489 - acc: 0.2523 - val_loss: 0.4487 - val_acc: 0.2523\n","Epoch 139/3500\n","5501/5501 [==============================] - 2s 446us/sample - loss: 0.4488 - acc: 0.2523 - val_loss: 0.4486 - val_acc: 0.2523\n","Epoch 140/3500\n","5501/5501 [==============================] - 2s 446us/sample - loss: 0.4487 - acc: 0.2523 - val_loss: 0.4486 - val_acc: 0.2523\n","Epoch 141/3500\n","5501/5501 [==============================] - 2s 450us/sample - loss: 0.4487 - acc: 0.2523 - val_loss: 0.4486 - val_acc: 0.2523\n","Epoch 142/3500\n","5501/5501 [==============================] - 2s 446us/sample - loss: 0.4487 - acc: 0.2523 - val_loss: 0.4487 - val_acc: 0.2523\n","Epoch 143/3500\n","5501/5501 [==============================] - 2s 446us/sample - loss: 0.4489 - acc: 0.2523 - val_loss: 0.4487 - val_acc: 0.2523\n","Epoch 144/3500\n","5501/5501 [==============================] - 2s 447us/sample - loss: 0.4488 - acc: 0.2523 - val_loss: 0.4487 - val_acc: 0.2523\n","Epoch 145/3500\n","5501/5501 [==============================] - 2s 445us/sample - loss: 0.4487 - acc: 0.2523 - val_loss: 0.4486 - val_acc: 0.2523\n","Epoch 146/3500\n","5501/5501 [==============================] - 2s 443us/sample - loss: 0.4488 - acc: 0.2523 - val_loss: 0.4487 - val_acc: 0.2523\n","Epoch 147/3500\n","5501/5501 [==============================] - 2s 450us/sample - loss: 0.4487 - acc: 0.2523 - val_loss: 0.4486 - val_acc: 0.2523\n","Epoch 148/3500\n","5501/5501 [==============================] - 2s 447us/sample - loss: 0.4487 - acc: 0.2523 - val_loss: 0.4487 - val_acc: 0.2523\n","Epoch 149/3500\n","5501/5501 [==============================] - 2s 444us/sample - loss: 0.4488 - acc: 0.2523 - val_loss: 0.4486 - val_acc: 0.2523\n","Epoch 150/3500\n","5501/5501 [==============================] - 2s 446us/sample - loss: 0.4489 - acc: 0.2523 - val_loss: 0.4488 - val_acc: 0.2523\n","Epoch 151/3500\n","5501/5501 [==============================] - 2s 447us/sample - loss: 0.4488 - acc: 0.2523 - val_loss: 0.4486 - val_acc: 0.2523\n","Epoch 152/3500\n","5501/5501 [==============================] - 2s 448us/sample - loss: 0.4487 - acc: 0.2523 - val_loss: 0.4488 - val_acc: 0.2523\n","Epoch 153/3500\n","5501/5501 [==============================] - 2s 449us/sample - loss: 0.4488 - acc: 0.2523 - val_loss: 0.4486 - val_acc: 0.2523\n","Epoch 154/3500\n","5501/5501 [==============================] - 2s 442us/sample - loss: 0.4487 - acc: 0.2523 - val_loss: 0.4486 - val_acc: 0.2523\n","Epoch 155/3500\n","5501/5501 [==============================] - 2s 447us/sample - loss: 0.4487 - acc: 0.2523 - val_loss: 0.4486 - val_acc: 0.2523\n","Epoch 156/3500\n","5501/5501 [==============================] - 2s 447us/sample - loss: 0.4489 - acc: 0.2523 - val_loss: 0.4487 - val_acc: 0.2523\n","Epoch 157/3500\n","5501/5501 [==============================] - 2s 446us/sample - loss: 0.4488 - acc: 0.2523 - val_loss: 0.4486 - val_acc: 0.2523\n","Epoch 158/3500\n","5501/5501 [==============================] - 2s 447us/sample - loss: 0.4487 - acc: 0.2523 - val_loss: 0.4486 - val_acc: 0.2523\n","Epoch 159/3500\n","5501/5501 [==============================] - 2s 445us/sample - loss: 0.4487 - acc: 0.2523 - val_loss: 0.4485 - val_acc: 0.2523\n","Epoch 160/3500\n","5501/5501 [==============================] - 2s 446us/sample - loss: 0.4488 - acc: 0.2523 - val_loss: 0.4487 - val_acc: 0.2523\n","Epoch 161/3500\n","5501/5501 [==============================] - 2s 448us/sample - loss: 0.4487 - acc: 0.2523 - val_loss: 0.4485 - val_acc: 0.2523\n","Epoch 162/3500\n","5501/5501 [==============================] - 2s 447us/sample - loss: 0.4487 - acc: 0.2523 - val_loss: 0.4486 - val_acc: 0.2523\n","Epoch 163/3500\n","5501/5501 [==============================] - 2s 448us/sample - loss: 0.4487 - acc: 0.2523 - val_loss: 0.4486 - val_acc: 0.2523\n","Epoch 164/3500\n","5501/5501 [==============================] - 2s 445us/sample - loss: 0.4487 - acc: 0.2523 - val_loss: 0.4486 - val_acc: 0.2523\n","Epoch 165/3500\n","5501/5501 [==============================] - 2s 449us/sample - loss: 0.4487 - acc: 0.2523 - val_loss: 0.4486 - val_acc: 0.2523\n","Epoch 166/3500\n","5501/5501 [==============================] - 2s 450us/sample - loss: 0.4487 - acc: 0.2523 - val_loss: 0.4486 - val_acc: 0.2523\n","Epoch 167/3500\n","5501/5501 [==============================] - 2s 450us/sample - loss: 0.4486 - acc: 0.2523 - val_loss: 0.4485 - val_acc: 0.2523\n","Epoch 168/3500\n","5501/5501 [==============================] - 2s 451us/sample - loss: 0.4487 - acc: 0.2523 - val_loss: 0.4486 - val_acc: 0.2523\n","Epoch 169/3500\n","5501/5501 [==============================] - 2s 446us/sample - loss: 0.4488 - acc: 0.2523 - val_loss: 0.4486 - val_acc: 0.2523\n","Epoch 170/3500\n","5501/5501 [==============================] - 2s 446us/sample - loss: 0.4487 - acc: 0.2523 - val_loss: 0.4487 - val_acc: 0.2523\n","Epoch 171/3500\n","5501/5501 [==============================] - 2s 449us/sample - loss: 0.4487 - acc: 0.2523 - val_loss: 0.4486 - val_acc: 0.2523\n","Epoch 172/3500\n","5501/5501 [==============================] - 2s 448us/sample - loss: 0.4487 - acc: 0.2523 - val_loss: 0.4487 - val_acc: 0.2523\n","Epoch 173/3500\n","5501/5501 [==============================] - 2s 450us/sample - loss: 0.4487 - acc: 0.2523 - val_loss: 0.4486 - val_acc: 0.2523\n","Epoch 174/3500\n","5501/5501 [==============================] - 2s 445us/sample - loss: 0.4487 - acc: 0.2523 - val_loss: 0.4485 - val_acc: 0.2523\n","Epoch 175/3500\n","5501/5501 [==============================] - 2s 445us/sample - loss: 0.4487 - acc: 0.2523 - val_loss: 0.4486 - val_acc: 0.2523\n","Epoch 176/3500\n","5501/5501 [==============================] - 2s 447us/sample - loss: 0.4487 - acc: 0.2523 - val_loss: 0.4486 - val_acc: 0.2523\n","Epoch 177/3500\n","5501/5501 [==============================] - 2s 447us/sample - loss: 0.4487 - acc: 0.2523 - val_loss: 0.4487 - val_acc: 0.2523\n","Epoch 178/3500\n","5501/5501 [==============================] - 2s 444us/sample - loss: 0.4486 - acc: 0.2523 - val_loss: 0.4485 - val_acc: 0.2523\n","Epoch 179/3500\n","5501/5501 [==============================] - 2s 445us/sample - loss: 0.4487 - acc: 0.2523 - val_loss: 0.4486 - val_acc: 0.2523\n","Epoch 180/3500\n","5501/5501 [==============================] - 2s 443us/sample - loss: 0.4487 - acc: 0.2523 - val_loss: 0.4486 - val_acc: 0.2523\n","Epoch 181/3500\n","5501/5501 [==============================] - 2s 446us/sample - loss: 0.4487 - acc: 0.2523 - val_loss: 0.4487 - val_acc: 0.2523\n","Epoch 182/3500\n","5501/5501 [==============================] - 2s 443us/sample - loss: 0.4487 - acc: 0.2523 - val_loss: 0.4485 - val_acc: 0.2523\n","Epoch 183/3500\n","5501/5501 [==============================] - 2s 445us/sample - loss: 0.4486 - acc: 0.2523 - val_loss: 0.4485 - val_acc: 0.2523\n","Epoch 184/3500\n","5501/5501 [==============================] - 2s 447us/sample - loss: 0.4486 - acc: 0.2523 - val_loss: 0.4485 - val_acc: 0.2523\n","Epoch 185/3500\n","5501/5501 [==============================] - 2s 445us/sample - loss: 0.4487 - acc: 0.2523 - val_loss: 0.4485 - val_acc: 0.2523\n","Epoch 186/3500\n","5501/5501 [==============================] - 2s 445us/sample - loss: 0.4486 - acc: 0.2523 - val_loss: 0.4485 - val_acc: 0.2523\n","Epoch 187/3500\n","5501/5501 [==============================] - 2s 450us/sample - loss: 0.4486 - acc: 0.2523 - val_loss: 0.4485 - val_acc: 0.2523\n","Epoch 188/3500\n","5501/5501 [==============================] - 3s 565us/sample - loss: 0.4487 - acc: 0.2523 - val_loss: 0.4487 - val_acc: 0.2523\n"],"name":"stdout"}]}]}